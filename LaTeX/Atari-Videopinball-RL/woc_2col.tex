%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
% $Id: woc_2col.tex 158 2017-01-19 23:08:23Z foley $
% $URL: https://repository.cs.ru.is/svn/template/tvd/journal/matec-woc/woc_2col.tex $
% 
% This is a template file for Web of Conferences Journal
%
% Copy it to a new file with a new name and use it as the basis
% for your article
%
% This template has been updated to match the Word Template's contents
% by Joseph T. Foley < foley AT RU dot IS >
%
%%%%%%%%%%%%%%%%%%%%%%%%%% EDP Science %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%\documentclass[option]{webofc}
%%% "twocolumn" for typesetting an article in two columns format (default one column)
%
\documentclass[twocolumn]{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font
\usepackage{booktabs}
\usepackage{array} %% needed for advanced table manipulation
%% Column types from http://tex.stackexchange.com/questions/54069/table-with-text-wrapping
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\graphicspath{{graphics/}{graphics/arch/}{Graphics/}{./}} % Look in these folders for graphics
%
% Put here some packages required or/and some personnal commands
\usepackage{svg}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
%
%
\begin{document}
%
\title{Implementing Deep Q-Learning for Autonomous Control in Atari Video Pinball}
%
% subtitle is optionnal
%
%%%\subtitle{Do you have a subtitle?\\ If so, write it here}

\author{\firstname{Luca} \lastname{Mohr}\inst{1}\and
        \firstname{Christian} \lastname{Schmid}\inst{1}\and
        \firstname{Niklas} \lastname{Scholz}\inst{1}
        % etc.
      }

\institute{Baden-Wuerttemberg Cooperative State University Mannheim, Facility of economics, 68163 Coblitzallee 1-9, Germany
          }

\abstract{
In this paper, we present an approach to train an agent to play the Atari Video Pinball game using Deep Q-Networks (DQN), a popular reinforcement learning algorithm. DQN leverages deep learning to approximate Q-values, which represent the expected future rewards of taking actions in particular states. We preprocess the game frames, employ an epsilon-greedy strategy for action selection, and utilize a replay memory to store and sample experiences for training. Our results demonstrate the agent's capability to learn effective policies through iterative interaction with the game environment. We discuss the training process, challenges encountered, and potential improvements. This study contributes to the field of reinforcement learning by providing insights into the application of DQN in complex environments like Atari games.
%
  %You should leave 8 mm of space above the abstract and 10 mm after the abstract.
  %The heading Abstract should be typed in bold 9-point Times.
  %The body of the abstract should be typed in normal 9-point Arial in a single paragraph, immediately following the heading.
  %The text should be set to 1 line spacing.
  %The abstract should be centred across the page, indented 17 mm from the left and right page margins and justified.
  %It should not normally exceed 200 words.
}
%
\maketitle
%

\section{Introduction}\label{sec:page-layout}

The rapid advancement of artificial intelligence (AI) and machine learning (ML) has significantly propelled the field of reinforcement learning (RL), especially through the development and application of Deep Q-Networks (DQN). DQN is a prominent RL algorithm that utilises deep learning techniques to approximate Q-values, which are critical for predicting the expected future rewards of taking specific actions in given states. This capability is essential for training agents to perform tasks that involve sequential decision-making and dynamic environments.

\begin{figure}[!h]
\centering
\centerline{\includegraphics[scale=0.17]{graphics/game.jpg}}
\caption{This figure presents a demo of the actual game.}
\label{fig: game}
\end{figure} 

This paper presents an approach to train an agent to play the Atari Video Pinball game using the DQN algorithm. Atari games are renowned for their complexity and serve as a challenging benchmark for assessing the performance and robustness of RL algorithms. The methodology employed includes the preprocessing of game frames to provide the agent with meaningful input data, the implementation of an epsilon-greedy strategy to balance exploration and exploitation during action selection, and the utilisation of a replay memory to store and sample experiences for effective training.

The experimental results demonstrate the agent’s capacity to learn effective policies through iterative interactions with the game environment. A detailed analysis of the training process, the challenges encountered, and potential improvements to enhance the agent’s performance is provided. This study contributes to the field of reinforcement learning by offering insights into the practical application of DQN in complex environments such as Atari games. It thus advances our understanding of the algorithm’s strengths and limitations in real-world scenarios.


\section{Related Work}

\subsection{Deep Q-Networks}

The work of Mnih et al. \cite{mnih2013playingatarideepreinforcement} led to the implementation of DQNs that successfully played Atari 2600 games, outperforming human experts in several games. The DQN architecture integrates a convolutional neural network (CNN) to approximate Q-values, and uses experience replay to reduce data correlation and stabilise the learning process. This methodology has been shown to be robust and effective, and has formed the basis for many subsequent studies in the field.

\subsection{Extensions of DQN}
Several extensions of the original DQN algorithm have been developed to further improve its performance and stability. Double DQN, introduced by Van Hasselt et al. \cite{van_Hasselt_Guez_Silver_2016}, aims to reduce Q-value overestimation by using two separate networks, one for action selection and another for Q-value calculation.

Another notable extension is the Dueling DQN by Wang et al. \cite{pmlr-v48-wangf16}, which decomposes the Q-value function into two separate estimates: one for the benefit of a given action and another for the value of the state. This separation allows the agent to better evaluate the relative quality of actions in a given state, leading to faster and more stable learning.

\subsection{Prioritized Experience Replay}
Schaul et al. \cite{schaul2016prioritizedexperiencereplay} introduced the concept of prioritised experience replay, where experiences are sampled based on their importance to learning rather than randomly. This technique improves training efficiency by ensuring that experiences that are more important and informative are replayed more often. 

\subsection{Hierarchical Reinforcement Learning}
Another approach to enhancing the efficiency and scalability of RL is Hierarchical Reinforcement Learning (HRL). Dietterich \cite{dietterich2000hierarchical} introduced the MAXQ framework, which decomposes the learning task into a hierarchy of subtasks. This hierarchy helps the agent tackle complex tasks by breaking down the learning objectives into more manageable steps.

\section{Atari Video Pinball Game}

"Atari Video Pinball", released by Atari, Inc. in 1980 for the Atari VCS (later known as the Atari 2600), is a video game that simulates the experience of playing a pinball machine. The game was developed by Bob Smith and incorporates essential elements of pinball, including flippers, bumpers, and a ball launcher. The player controls the aforementioned elements using the Atari joystick, thereby replicating the physical actions involved in traditional pinball gameplay. The movement of the joystick allows for the activation of the left and right flippers, while a downward pull on the joystick emulates the action of pulling back a plunger to launch the ball into play. \cite{newman2018atari}

The game is designed to emulate the dynamics of an arcade pinball machine, with digital enhancements. For example, the player is rewarded with an additional ball upon hitting the Atari logo on the playfield four times, thereby introducing an engaging element to the gameplay. Furthermore, the game incorporates features such as ball nudging, which enables players to subtly adjust the ball’s trajectory by holding down the joystick button and moving the controller. However, excessive nudging results in a state of tilt, which causes the ball to drain and simulates the response of a real pinball machine to excessive manipulation. \cite{newman2018atari}

The gameplay mechanics of Atari Video Pinball entail a single-player mode in which players alternate control of the ball in order to achieve the highest possible score. The game's visual design and scoring system were innovative for their time, offering a digital representation of the physical pinball experience. It represents a notable example of early video game design, wherein developers sought to bring popular arcade experiences into the home environment. \cite{newman2018atari}

In conclusion, "Atari Video Pinball" represents a notable early instance of video gaming that effectively translated the excitement and challenge of pinball into a home console format. The innovative use of the Atari joystick for game control and the engaging gameplay mechanics contribute to the significance of this title in the history of video gaming. \cite{newman2018atari}

\section{Configuration and simplifications}
In our study, we utilized the ALE/VideoPinball-v5 environment from the Gymnasium library for training and evaluating our reinforcement learning models. This environment offers several advantages in terms of the richness of visual data and the ability to preprocess this data effectively for deep learning tasks. \cite{gymnasium}

The ALE/VideoPinball-v5 environment produces RGB images as its primary output. These RGB frames provide a detailed visual representation of the game state, capturing all the graphical nuances and spatial information essential for the agent to understand the environment fully. We opted to use these RGB frames initially and then convert them to grayscale. Converting the frames to grayscale reduces computational complexity by eliminating color information, which is often redundant for the task, while still retaining the essential structural details. This approach balances the need for detailed visual information with computational efficiency, making it more effective than using the raw 128-byte RAM data, which is highly abstracted and lacks the rich context provided by image data. \cite{mnih2013playingatarideepreinforcement}

For the action space, we used the Discrete(9) configuration, which includes nine possible actions the agent can take. This reduced action space simplifies the learning process by limiting the number of actions the agent must evaluate and choose from at each step. Using the full Discrete(18) action space, while providing a more granular control, significantly increases the complexity and computational burden of the learning process. By using Discrete(9), we strike a balance between providing the agent with sufficient action choices to perform effectively and maintaining a manageable level of complexity for our computational resources. \cite{gymnasium}

We employed the standard difficulty level for our experiments. The standard difficulty level provides a balanced challenge that allows for effective training and evaluation of the agent's capabilities. In contrast, the hard difficulty level increases the game's complexity, making it significantly more challenging for the agent to learn and perform well. Given the constraints of our computational resources, including the limited training epochs and inability to perform extensive hyperparameter optimization, the standard difficulty level was chosen to ensure the agent could achieve meaningful learning within these constraints. \cite{gymnasium} 


\section{An overview of the fundamental principles of the $\epsilon$-greedy method.}\label{sec:fig-tables}
The $\epsilon$-greedy method represents a straightforward and prevalent approach to addressing the exploration-exploitation dilemma in reinforcement learning. It integrates two pivotal strategies: exploitation (the utilisation of current knowledge to maximise immediate benefits) and exploration (the investigation of novel actions with the objective of enhancing long-term outcomes).
\subsection{Functionality}
In the $\epsilon$-greedy procedure, the action with the highest estimated value (i.e. the greedy action) is typically selected. However, a random action is selected with a small probability $\epsilon$. This random selection allows the agent to try out seemingly sub-optimal actions, which can lead to better insights in the long term.
The action $a_t$ in a given state $s_t$ is selected according to the following procedure:
\begin{equation}
a_t =
  \begin{cases} 
   \text{argmax}_a Q(s_t, a) & \text{with probability } 1 - \epsilon \\
   \text{random action from A} & \text{with probability } \epsilon 
  \end{cases}
\end{equation}
The estimated reward for the action $a$ in state $s_t$ is given by $Q(s_t, a)$. The set of all possible actions is denoted by $A$.
The \textbf{advantages} of the $\epsilon$-greedy process are as follows. Firstly, the process guarantees exploration. Secondly, over time, each action is executed infinitely often, which ensures that all $Q(s_t, a)$ converge to their true values $q^*(s_t, a)$. \cite{sutton-2018} Thirdly, the method is simple to implement and does not require complicated calculations or models of the environment.\newline
One of the main \textbf{disadvantages} of the procedure is that it also selects suboptimal actions, which can lead to lower rewards in the short term. Additionally, the $\epsilon$-greedy process is non-adaptive. A constant value of $\epsilon$ can be suboptimal in different phases of the learning process. One way to improve this is to reduce the value of $\epsilon$ over time.

\subsection{Technical depth and formulas}
The $\epsilon$-greedy method is based on the estimation of the Q values, which can be adjusted by the following update step:

\begin{equation}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
\end{equation}

Here is:
The notation $Q(s_t, a_t)$ represents the current estimated value of the action $a_t$ in the state $s_t$. The notation $r_{t+1}$ denotes the reward received after performing $a_t$ in state $s_t$. The notation $\alpha$ represents the learning rate, while the notation $\gamma$ denotes the discount factor that devalues future rewards.


\subsection{Practical effectiveness and applications}
In practice, the $\epsilon$-greedy method has been demonstrated to be effective, particularly in cases where the environment is stationary or only minor changes occur. It is particularly useful in the early stages of learning, when the agent still knows little about the environment and exploratory behaviour should be encouraged.
In applications such as display optimisation on web pages (e.g. personalised web services), the $\epsilon$-greedy method is employed to maximise both short-term clicks and to enhance long-term user interactions. \cite{sutton-2018}

\section{Deep Q-Network}
Deep Q-Networks (DQN) represent an advance in reinforcement learning (RL) through the integration of deep learning with Q-learning, enabling the handling of high-dimensional sensory inputs. This model is introduced by Mnih et al. \cite{mnih2013playingatarideepreinforcement} and represents the inaugural successful framework that learns control policies directly from raw pixels using RL techniques. For example, DQN was applied to to Atari 2600 games in the Arcade Learning Environment (ALE) and demonstrated superior performance compared to to previous methods, achieving significant gains in six out of seven games and surpassing human experts in three. \cite{mnih2013playingatarideepreinforcement}

In the past, reinforcement learning with high-dimensional inputs such as speech was typically approached through the use of handcrafted features combined with linear value functions. However, these approaches were constrained by the quality of the feature representation. The significant advances in deep learning, particularly convolutional neural network (CNN) have enabled the extraction of high-level features from raw data, leading to notable success in computer vision and speech recognition. \cite{mnih2013playingatarideepreinforcement}

RL presents a number of distinctive challenges for deep learning, including the learning of sparse and noisy reward signals, the presence of high data correlation, and the necessity of dynamic data distributions. DQN addresses these challenges by integrating a CNN with a variant of the Q-learning algorithm and utilising experience replay to stabilise the training process. The agent interacts with the environment, selecting actions with the objective of maximising future rewards based on sequences of observed images. \cite{mnih2013playingatarideepreinforcement}

\subsection{Q-Learning}

\begin{algorithm}[H]
\caption{Deep Q-learning with Experience Replay}
\begin{algorithmic}[1]
\State Initialize replay memory $\mathcal{D}$ to capacity $N$
\State Initialize action-value function $Q$ with random weights
\For{episode $= 1$ to $M$}
    \State Initialise sequence $s_1 = \{ x_1 \}$ and preprocessed \Statex \quad \quad sequence $\phi_1 = \phi(s_1)$
    \For{$t = 1$ to $T$}
        \State With probability $\epsilon$ select a random action $a_t$
        \State otherwise select $a_t = \arg\max_a Q^*(\phi(s_t), a; \theta)$
        \State Execute action $a_t$ in emulator and observe \Statex \quad \quad \quad reward $r_t$ and image $x_{t+1}$
        \State Set $s_{t+1} = s_t, a_t, x_{t+1}$ and \Statex \quad \quad \quad preprocess $\phi_{t+1} = \phi(s_{t+1})$
        \State Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in $\mathcal{D}$
        \State Sample random minibatch of transitions \Statex \qquad \quad $(\phi_j, a_j, r_j, \phi_{j+1})$ from $\mathcal{D}$
        \State $y_j = \begin{cases} 
        r_j & \text{terminal } \phi_{j+1} \\
        r_j + \gamma \max_{a'} Q(\phi_{j+1}, a'; \theta) & \text{otherwise}
        \end{cases}$
        \State Perform a gradient descent step on \Statex \qquad \quad $(y_j - Q(\phi_j, a_j; \theta))^2$ according to equation 3
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}


The Q-learning algorithm updates the action-value function iteratively using the Bellman equation, which is based on the following intuition: if the optimal value $Q(s', a')$ of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to select the action $a'$ maximizing the expected value of $r + \gamma Q(s', a')$: \cite{mnih2013playingatarideepreinforcement}

\begin{equation}
    Q(s, a) = \mathbb{E}_{s' \sim E} \left[ r + \gamma \max_{a'} Q(s', a') \mid s, a \right]
\end{equation}

A neural network function approximator, known as a Q-network, is used to estimate $Q(s, a; \theta) \approx Q^*(s, a)$, where $\theta$ represents the network weights. The network is trained by minimizing a sequence of loss functions $L_i(\theta_i)$, which measure the difference between predicted and target Q-values \cite{mnih2013playingatarideepreinforcement}:

\begin{equation}
    L_i(\theta_i) = \mathbb{E}_{(s,a) \sim \rho(\cdot)} \left[ \left( y_i - Q(s, a; \theta_i) \right)^2 \right]
\end{equation}

where the target for iteration $i$ is

\begin{equation}
    y_i = \mathbb{E}_{s' \sim E} \left[ r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) \mid s, a \right]
\end{equation}

The gradient of the loss function with respect to the weights is

\begin{equation}
\begin{aligned}
\nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s, a \sim \rho(\cdot); s' \sim \mathcal{E}} \Bigg[ \left( r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) \right. \\
\left. - Q(s, a; \theta_i) \right) \nabla_{\theta_i} Q(s, a; \theta_i) \Bigg]
\end{aligned}
\end{equation}

\begin{comment}
\begin{equation}
    \nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{(s,a) \sim \rho(\cdot); s' \sim E} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) - Q(s, a; \theta_i) \right) \nabla_{\theta_i} Q(s, a; \theta_i) \right]
\end{equation}
\end{comment}

DQN employs experience replay in order to address correlated data and non-stationary distributions. The algorithm employs a process of storing and randomly sampling the agent’s experiences, thereby enabling the reuse of experiences on multiple occasions. This approach serves to enhance the efficiency of the data and the stability of the learning process. The neural network processes the raw Atari frames, converting them to grayscale and downsampling them to an 84×84 image. The network comprises three hidden layers: two convolutional layers, followed by rectifiers, and a fully connected layer with 256 units. The output layer is responsible for predicting Q-values for all possible actions in a given state. \cite{mnih2013playingatarideepreinforcement}

The DQN algorithm was tested on seven Atari games: Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest, and Space Invaders. In all seven games, the same architecture and hyperparameters were used. In order to limit the error scale, rewards were clipped. Furthermore, the RMSProp algorithm with minibatches of size $32$ was employed. The $\epsilon$-greedy policy strikes a balance between exploration and exploitation. The stability of the training process was demonstrated by the tracking of the average maximum predicted action-value function $Q$ throughout the training period. \cite{mnih2013playingatarideepreinforcement}

The DQN method demonstrated superior performance to existing RL methods and human experts on several games, indicating its capacity to learn directly from raw inputs without prior knowledge. The advent of DQNs represents a pivotal moment in the integration of deep learning and reinforcement learning. By effectively merging Q-learning with deep convolutional neural networks (CNNs) and experience replay, this approach has set a new standard for reinforcement learning (RL) algorithms, achieving state-of-the-art results on a variety of Atari games. This success serves to illustrate the potential of deep learning techniques in addressing complex RL tasks, thereby paving the way for future advancements in the field. \cite{mnih2013playingatarideepreinforcement}

% \begin{equation}
% Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right].
% \end{equation}



%\vspace{1.5\baselineskip}
\section{Implementation}
\subsection{Baseline}

The initial phase of the baseline method involves executing a specific sequence of actions designed to ensure the ball enters the playfield.

Following this introductory sequence, the Agent continues to trigger both flippers continuously. This approach embodies a naive "spam" strategy, similar to the behavior one might expect from a child repeatedly pressing the same button. The rationale for this method is twofold. First, it simulates a simplistic and unsophisticated play style, thereby establishing a minimal performance benchmark. Second, it is computationally efficient since the action does not require complex calculations but rather involves executing the same command repeatedly.

The episode concludes based on the game’s standard termination signals or a custom condition that checks for the ball being stuck. If the ball's position remains unchanged for 30,000 frames (approximately 5 seconds), the episode terminates to prevent prolonged inactivity.

This baseline was evaluated over three epochs, with the cumulative score recorded for each episode. The mean score across these epochs serves as the benchmark for future comparison. The simplicity and reproducibility of this baseline make it an ideal starting point for further experimentation. Moreover, it provides a clear, quantifiable measure against which more advanced, sophisticated models can be assessed.

\subsection{Preprocessing}
To reduce the computational costs associated with processing frames of \(210 \times 160\) pixels and a 128-color palette that the Atari environment returns, the frames were first converted from RGB to greyscale and then cropped to \(84 \times 84\) pixels. This preprocessing step significantly reduces the dimensionality of the input data, thereby decreasing computational complexity and memory usage. 

Additionally, four consecutive frames were stacked together to form the input to the Deep Q-Network (DQN). This stacking of frames captures the temporal dynamics of the game environment, allowing the network to understand motion and predict future states based on past observations. By providing a sequence of frames as input, the DQN can learn more complex behaviors and strategies by comprehending how the game evolves over time.

The optimal action is then determined based on the last frame in the stack and applied uniformly to all frames. This approach not only reduces the computational effort by minimizing the frequency of action selection but also maintains the temporal coherence necessary for effective decision-making in the game environment.
\subsection{Network architecture}
\begin{figure}[!h]

\includesvg[scale=0.19]{graphics/nn-3.svg}
\caption{Architecture of the concolutional layers}
\label{fig: CNN}
\end{figure}

The network architecture comprises three convolutional layers, each followed by a batch normalization layer to stabilize and accelerate the training process by reducing internal covariate shift \cite{ioffe2015batch}. The detailed configuration of these layers is as follows: The first convolutional layer takes four input channels, corresponding to stacked frames, and outputs 32 channels using an $8\times8$ kernel with a stride of 4. This layer is responsible for capturing coarse spatial features from the input frames. The output is then normalized by the first batch normalization layer. The second convolutional layer takes 32 input channels and outputs 64 channels using a $4\times4$ kernel with a stride of 2. This layer further refines the spatial details and patterns extracted by the first layer, and its output is normalized by the second batch normalization layer. The third convolutional layer takes 64 input channels and outputs 64 channels using a $3\times3$ kernel with a stride of 1. This layer focuses on extracting fine-grained features from the previously processed feature maps, and its output is normalized by the third batch normalization layer. Each convolutional layer utilizes the ReLU activation function, introducing non-linearity into the model and enabling it to learn complex patterns. \cite{nair2010rectified}

Following the convolutional and normalization layers, the feature maps are flattened and passed through a fully connected layer. The input to this layer is calculated as the product of the height, width, and number of channels of the final feature maps, resulting in a feature vector that feeds into the fully connected layer. The input size is determined by the dimensions of the feature maps after the last convolutional layer. This layer outputs the Q-values for each of the 9 possible actions, representing the expected future rewards for taking each action from the current state. Thus, the output size is 9. During the forward pass, the input tensor, which consists of a stack of four consecutive frames, undergoes the following transformations: convolution, batch normalization, and ReLU activation in the first layer; convolution, batch normalization, and ReLU activation in the second layer; convolution, batch normalization, and ReLU activation in the third layer; flattening of the feature maps; and finally, processing through the fully connected layer to produce the Q-values. Importantly, this architecture allows for the computation of Q-values for all 9 possible actions with a single forward pass through the network.


\subsection{Training and Optimization}
The training process leverages a replay memory to store experiences in the form of (state, action, next\_state, reward) tuples. The replay memory has a capacity of 50,000, allowing it to retain a significant history of the agent's interactions with the environment. These experiences are sampled randomly in batches of 128 to break the temporal correlations and are used to train the network, which stabilizes the learning process. Various batch sizes (32, 64, 128, 256) were tested, and it was observed that a batch size of 128 resulted in the best performance in terms of minimal loss and time taken, as shown in the respective table \ref{tab: batch sizes}. This method ensures that the distribution of experiences used for learning better represents the true environment dynamics, thereby enhancing the stability and efficiency of the training. The optimization of the network parameters is done using the AdamW optimizer with a learning rate of $1 \times 10^{-4}$, which helps in mitigating the issues of vanishing and exploding gradients.

\begin{table}[!h]
    \centering
    \begin{tabular}{rrr}
        \toprule
         \textbf{batch size} & \textbf{average loss per epoch} & \textbf{time taken} 
         \\ \midrule
          32 & 5.58  & 119.43s \\ \addlinespace
         64 & 5.93 & 172.96s \\ \addlinespace
        128 & 4.37 & 93.42s \\ \addlinespace
         256 & 4.54 & 168.93s \\ \addlinespace
         \bottomrule
    \end{tabular}
    \caption{The table illustrates the average loss over ten epochs and the time taken for different batch sizes. A batch size of 128 achieves the lowest average loss of 4.37 with the shortest training time of 93.42 seconds, indicating optimal performance and efficiency. In contrast, increasing the batch size to 256 results in a slightly higher average loss of 4.54 and a significantly longer training time of 168.93 seconds, highlighting the trade-off between batch size and computational load. Smaller batch sizes, such as 32 and 64, exhibit higher average losses and longer training times compared to the optimal batch size of 128.}
    \label{tab: batch sizes}
\end{table}

%The training process leverages a replay memory to store experiences in the form of (state, action, next\_state, reward) tuples. These experiences are sampled randomly to break the temporal correlations and are used to train the network, which stabilizes the learning process. The optimization of the network parameters is done using the AdamW optimizer with a learning rate of $1 \times 10^{-4}$, which helps in mitigating the issues of vanishing and exploding gradients.



A target network is used alongside the policy network to compute the target Q-values. The weights of the target network are updated periodically using a soft update mechanism controlled by the parameter $\tau = 0.005$. This approach prevents the oscillations and divergence often observed when the policy network is directly used to estimate the target Q-values.

The loss function used is Smooth L1 Loss, which is less sensitive to outliers compared to the mean squared error loss. This choice is crucial for stabilizing the training process, as it balances the robustness and sensitivity required for accurate Q-value estimation.



The Deep Q-Network (DQN) agents were trained over a total of 500 epochs, with each epoch comprising 3 episodes, resulting in a cumulative training set of 1500 episodes. Throughout the training process, key performance metrics were recorded at the end of each epoch. These metrics included the average reward per epoch, the average duration per epoch, the loss values, and the epsilon decay. Epsilon decay follows an exponential decay rule, defined as

\begin{equation}
\epsilon = \epsilon_{\text{end}} + (\epsilon_{\text{start}} - \epsilon_{\text{end}}) \times \exp\left(-\frac{\text{steps\_done}}{\epsilon_{\text{decay}}}\right),
\end{equation}

where \(\epsilon_{\text{start}} = 0.9\), \(\epsilon_{\text{end}} = 0.05\), and \(\epsilon_{\text{decay}} = 1000\). This rule ensures a gradual transition from exploration to exploitation as training progresses. Post-training, these metrics were plotted to visualize the agent's learning progress and performance trends. The resulting graphs are presented in Figures \ref{fig: episode_duration} through \ref{fig: loss}.


\begin{figure}[!h]
\centering
\includesvg[scale=0.4]{graphics/epoch_duration.svg}
\caption{Epoch duration plot during the training of the policy network (DQN). The x-axis represents the epoch number, and the y-axis shows the duration in seconds. The blue line shows the raw duration per epoch, and the orange line is the moving average of the last 50 epochs. Initial high variability and significant spikes suggest initial inefficiencies, which stabilize over time. The moving average smooths short-term fluctuations, highlighting longer-term trends essential for diagnosing and optimizing training performance.}


\label{fig: episode_duration}
\end{figure}

\begin{figure}[!h]
\centering
\centerline{\includesvg[scale=0.4]{graphics/epoch_reward.svg}}
\caption{Epoch reward plot during the training of the policy network (DQN). The x-axis represents the epoch number, and the y-axis shows the reward. The blue line indicates the raw reward per epoch, while the orange line represents the moving average of the rewards. The plot shows initial high variability in rewards with significant peaks, which gradually decrease as training progresses. This pattern indicates the learning process of the model, with early exploration leading to high variability and later stabilization as the model converges. The moving average line smooths out short-term fluctuations and highlights longer-term trends, aiding in evaluating and optimizing the training performance.}
\label{fig: episode_reward}
\end{figure}

\begin{figure}[!h]
\centering
\centerline{\includesvg[scale=0.4]{graphics/epsilon_decay.svg}}
\caption{This figure depicts the decay of the epsilon value over training steps. The x-axis shows the number of steps, and the y-axis represents the epsilon value. The rapid initial decay promotes exploration, gradually shifting to exploitation as epsilon decreases.}
\label{fig: epsilon_decay}
\end{figure}

\begin{figure}[!h]
\centering
\centerline{\includesvg[scale=0.4]{graphics/loss.svg}}
\caption{This figure presents the loss values during optimization. The x-axis denotes optimization steps, and the y-axis shows the loss. The loss initially increases with high variability, then gradually decreases, indicating improving prediction accuracy of the Q-values.}
\label{fig: loss}
\end{figure}


\subsection{Comparison}
Lastly, the performance of the baseline method and the final model was compared. For this comparison, both models were evaluated over 10 epochs. The results of this comparison are presented in Table \ref{tab: results}. This table visualizes the rewards obtained by each model, providing a clear depiction of their respective performances. The comparison underscores the significant improvement achieved by the final model over the baseline.

\begin{table}[!h]
    \centering
    \begin{tabular}{lr}
        \toprule
         \textbf{Model} & \textbf{Average Reward per Epoch} 
         \\ \midrule
         Baseline & 2931.6  \\ \addlinespace
         DQN & 12078.8 \\ \addlinespace
         \bottomrule
    \end{tabular}
    \caption{Comparison of average rewards per epoch for baseline and DQN models in the Atari Video Pinball environment. The DQN model significantly outperforms the baseline model, achieving a substantially higher average reward per epoch. These models were compared over 5 epochs, which resulted in a total of 15 episodes.}
    \label{tab: results}
\end{table}


\section{Limitations and future research}
The preprocessing steps implemented, including converting frames to greyscale, cropping to \(84 \times 84\) pixels, and stacking four consecutive frames, have significantly reduced computational costs and improved training efficiency for the Atari Video Pinball environment. However, several limitations are inherent to this approach.

Converting frames to greyscale may lead to the loss of valuable color information that could potentially aid in the game's decision-making process. For instance, certain visual cues that rely on color differentiation are lost, potentially impacting the agent's ability to make informed decisions.

The fixed cropping to \(84 \times 84\) pixels, while reducing dimensionality, may exclude critical parts of the game environment. In Atari Video Pinball, where the entire playing field and paddle positions are essential, this fixed cropping might omit relevant areas, impacting the agent's performance.

Moreover, while stacking four frames helps capture temporal dynamics, it is a simplistic method that may not fully represent the intricate temporal dependencies present in the game. The fixed frame stack size introduces a hyperparameter that requires careful tuning, which was not extensively optimized due to limited computational resources.

Training the model effectively necessitated the use of GPU resources, which were not available locally. Consequently, we relied on Google Colab, where computational resources are limited. This constraint necessitated limiting the training to only 500 epochs and precluded extensive hyperparameter tuning, such as grid search. Only the batch size was varied and benchmarked in a brief trial, which may not have identified the optimal hyperparameters for the best performance.



Future research should focus on addressing these limitations to enhance the performance and efficiency of deep reinforcement learning models for Atari Video Pinball. One potential direction is to explore techniques that selectively incorporate color information, allowing the model to leverage both greyscale and color data where beneficial. 

Adaptive cropping methods could dynamically focus on the most relevant regions of each frame, ensuring that critical areas are retained and important contextual information is preserved. These methods could involve attention mechanisms or saliency detection to identify and prioritize crucial parts of the game environment.

To better capture temporal dependencies, employing more sophisticated architectures such as Long Short-Term Memory (LSTM) networks or Temporal Convolutional Networks (TCNs) could be beneficial. These models can handle longer and more complex temporal sequences beyond the fixed frame stack, potentially improving the agent's understanding of the game's dynamics.

Further research could also explore more nuanced action selection strategies within frame stacks. Predictive models that account for expected changes in the environment over the skipped frames could lead to more accurate and context-aware decision-making.

Lastly, it is crucial to investigate the scalability and generalizability of these methods across a broader range of games and environments. Robust and adaptable models are essential for ensuring that the developed techniques can be effectively applied to various tasks and scenarios. Moreover, securing access to more powerful computational resources for extensive training and hyperparameter optimization could significantly enhance the performance of the reinforcement learning models.



\bibliography{references}
\end{document}
